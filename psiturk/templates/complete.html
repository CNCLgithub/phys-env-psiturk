<!DOCTYPE html>

<html>
	<head>
        <meta charset="utf-8" />
		<title>Psychology Experiment</title>
		<link rel="stylesheet" href="/static/css/bootstrap.min.css" type="text/css" />
		<link rel="stylesheet" href="/static/css/style.css" type="text/css" />
	</head>
	<body>
		<div id="container-not-an-ad" class="media">
			<div class="media-left pull-left" href="#">
						<!-- REPLACE THE LOGO HERE WITH YOUR  UNIVERSITY, LAB, or COMPANY -->
						<img id="adlogo" src="/static/images/university.png" alt="Lab Logo" />
			</div>
			<div class="media-body">
			<h1>Study Debriefing</h1>
			<hr />
			<div class="alert alert-warning">
		<p>From a quick glance, the touch of an object, or a brief sound snippet, our brains construct scene representations composed of rich and detailed shapes and surfaces. These representations are not only the targets of perception, but also support aspects of cognition including thinking about how objects move and planning actions to manipulate them -- as in the exemplary case of using or making tools. How is it that perception transforms raw sensory signals arising from our physical environments, into things like objects and people, into things that we can think about? Ultimately, our lab is seeking a computational account of how the mind and brain might be performing such transformations.  </p>

		<p>In this study you made judgments about aspects of the physical world, such as the 3D shapes of objects, how they move, and how we can interact with and manipulate them. For each trial, we are interested in how accurate individual human subjects were and how they performed it (e.g., the time it took for them to respond on a given trial, where they looked on the stimuli, how they interacted with the object, etc.). Measuring these behavioral responses in adult human subjects help reveal behavior-level constraints over the nature of the computational substrate supporting the perception and cognition of objects and people.  There are many possible computational models that one can build and each will make different predictions as to how humans should perform on a given task. We compare the predictions arising from these models with the behavior of our subjects. We screen all alternative models in this way to find those that are most like humans in their accuracy and in the way they perform the task. </p>
		
		<p>The computational models we explore come from two somewhat opposing approaches to the study of the mind and brain. One of these approaches argues that the way we see and think about our physical world is based on the similar experiences we had in the past: We constantly build associations between various sensations and their outcomes that we experienced in the past, and those associations guide our perceptions and thoughts at the moment. </p>
		
		<p>The alternative approach argues that we see and think about the world by building small-scale models of the real world in our minds. These small-scale models provide a flexible medium to the mind that enables thoughts that go beyond the available sensations such as imagining previously unseen scenes. </p>
				
		<p> Drawing upon a recently developed technical toolkit, we build computational models representing both traditions and test them in behavioral data (as well as in neural data in other experiments) so to advance toward the main promise of our research, which is to build a formal understanding of the mind and brain. Moreover, making progress towards this goal should inspire building new, more human-like, and interpretable machine intelligences. Often in our research, we find that it is only through complex and surprising combinations of both of these traditions that we can explain human-level and human-like performance in our tasks.</p>
		
		<p>If you want to learn more about how we see, think about, and interact with objects, you may want to consult these two citations:</p>
		
		<p> Yildirim, I., Wu, J., Kanwisher, N., & Tenenbaum, J. (2019). An integrative computational architecture for object-driven cortex. <i>Current Opinion in Neurobiology, 55,</i> 73-81.</p>
		<p> Yildirim, I., Siegel, M., Tenenbaum, J. (2020). Physical object representations for perception and cognition. <i>The Cognitive Neurosciences, 6th Edition,</i> Gazzaniga, Mangun, Poeppel (Editors).</p>
		
		<p> If you are interested in learning more about this study, please contact Ilker Yildirim, 203-436-9229, ilker.yildirim@yale.edu. </p>
		<p> If you have concerns about your rights as a participant in this experiment, please contact the Human Subject Committee, 203-785-4688, human.subjects@yale.edu.</p>
			
	<script>
		console.log("exiting full screen");

		if (document.exitFullscreen) {
			document.exitFullscreen();
		} else if (document.webkitExitFullscreen) { /* Safari */
			document.webkitExitFullscreen();
		} else if (document.msExitFullscreen) { /* IE11 */
			document.msExitFullscreen();
		}
	</script>
                            <a href="https://yale.sona-systems.com/main.aspx">Click here to return to the SONA subject pool.</a> 
					    </div>
			</div>
		</div>
	</body>
</html>